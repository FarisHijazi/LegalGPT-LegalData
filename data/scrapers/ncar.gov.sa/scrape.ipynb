{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import datetime\n",
    "\n",
    "\n",
    "cookies = {\n",
    "    'XSRF-TOKEN': 'eyJpdiI6IjZoWTJPQk5sRXdPVUprL3pFNDVXdHc9PSIsInZhbHVlIjoiNHJMNU5kcXpPVGNDUWN1aHdFTitZanU3d3hpeWxub1dOdnoyZHZ0UmQ2eVhpOS9nVTErWnVvMEJJY3lScGI5d3k4RVZmY05KTzkzSkxBNTJpVjFseWhiQmh3UW9oeE5LQnJ6U0J3U2NLUjhZWkREQ1BNRXRVNG8za0h0VkIzZ2kiLCJtYWMiOiJiMDE5MTYwOGEzOGQ0MzAyNTcxMjQ4MjQzOGJhMDExNzQ4MDlmNWY2MTYxOWJjYWVjNDMyOTNmZTg4ODdjZWU4IiwidGFnIjoiIn0%3D',\n",
    "    'laravel_session': 'eyJpdiI6Illmdzk1MWFYVmtKN3kzY2FVaDVxU1E9PSIsInZhbHVlIjoiWEhlS0E3VnRINHluaUZkTC93MVR0WnQ0S3Fyd2NHa1hCcTBvaVBHMG9PaXJiczFSdktObXhMWFkxZHl2VkltaDlCQVphcVRiM2cxeGhaR3UwUXkrdkZFdE5sVVJQUk5oUEMwMzhvcHUvR2NUNk5keXpYcWhPL1JCSCtYM2lsaS8iLCJtYWMiOiIzZDQ5YWI0YTZlZmE0MjVmMWYyMmYzOGVmMmYzZjFkYTE2YmY3YTM1NTk3MWZhZmI1YTk0YjkzYTZjNjlmMWU1IiwidGFnIjoiIn0%3D',\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'cache-control': 'max-age=0',\n",
    "    # 'cookie': 'XSRF-TOKEN=eyJpdiI6IjZoWTJPQk5sRXdPVUprL3pFNDVXdHc9PSIsInZhbHVlIjoiNHJMNU5kcXpPVGNDUWN1aHdFTitZanU3d3hpeWxub1dOdnoyZHZ0UmQ2eVhpOS9nVTErWnVvMEJJY3lScGI5d3k4RVZmY05KTzkzSkxBNTJpVjFseWhiQmh3UW9oeE5LQnJ6U0J3U2NLUjhZWkREQ1BNRXRVNG8za0h0VkIzZ2kiLCJtYWMiOiJiMDE5MTYwOGEzOGQ0MzAyNTcxMjQ4MjQzOGJhMDExNzQ4MDlmNWY2MTYxOWJjYWVjNDMyOTNmZTg4ODdjZWU4IiwidGFnIjoiIn0%3D; laravel_session=eyJpdiI6Illmdzk1MWFYVmtKN3kzY2FVaDVxU1E9PSIsInZhbHVlIjoiWEhlS0E3VnRINHluaUZkTC93MVR0WnQ0S3Fyd2NHa1hCcTBvaVBHMG9PaXJiczFSdktObXhMWFkxZHl2VkltaDlCQVphcVRiM2cxeGhaR3UwUXkrdkZFdE5sVVJQUk5oUEMwMzhvcHUvR2NUNk5keXpYcWhPL1JCSCtYM2lsaS8iLCJtYWMiOiIzZDQ5YWI0YTZlZmE0MjVmMWYyMmYzOGVmMmYzZjFkYTE2YmY3YTM1NTk3MWZhZmI1YTk0YjkzYTZjNjlmMWU1IiwidGFnIjoiIn0%3D',\n",
    "    'dnt': '1',\n",
    "    'if-modified-since': 'Mon, 26 Feb 2024 10:48:43 GMT',\n",
    "    'if-none-match': '\"8037965da168da1:0\"',\n",
    "    'sec-ch-ua': '\"Chromium\";v=\"123\", \"Not:A-Brand\";v=\"8\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"macOS\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    # 'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',\n",
    "    \"authorization\": \"bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3MTQ1NTU3MTIsInN1YiI6IjkzZjE5MTJiLTQzNzEtNGEyYS05NTg2LWUzM2RjMzYxMDNlZCJ9.NjV4YYREnf8RawkyEaDuneBTnvjmoovlBTl6bhomgbo\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "\n",
    "def hash_str(input_string: str):\n",
    "    return hashlib.sha256(input_string.encode('utf-8')).hexdigest()\n",
    "\n",
    "def json_remove_keys(json_obj, keys):\n",
    "    if isinstance(json_obj, dict):\n",
    "        return {k: json_remove_keys(v, keys) for k, v in json_obj.items() if k not in keys}\n",
    "    if isinstance(json_obj, list):\n",
    "        return [json_remove_keys(v, keys) for v in json_obj]\n",
    "    return json_obj\n",
    "\n",
    "def hash_json(json_obj):\n",
    "    # Serialize JSON object with sorted keys\n",
    "    serialized_json = json.dumps(json_obj, sort_keys=True).encode('utf-8')\n",
    "    return hashlib.sha256(serialized_json).hexdigest()\n",
    "\n",
    "def gen_id(item):\n",
    "    return hash_json({\n",
    "        \"title_en\": item['title_en'],\n",
    "        \"title_ar\": item['title_ar'],\n",
    "        \"number\": item['number'],\n",
    "        \"approve_date\": item['Approves'][0]['approve_date'],\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists('ar_i18n.json'):\n",
    "    response = requests.get('https://ncar.gov.sa/assets/i18n/ar.json', headers=headers)\n",
    "    with open('ar_i18n.json', 'w') as f:\n",
    "        f.write(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = requests.post(\"https://ncar.gov.sa/api/index.php/api/documents/all_release_orgnizations/1/1\", headers=headers, cookies=cookies)\n",
    "partial_data = response.json()\n",
    "print('partial_data', len(partial_data), partial_data)\n",
    "\n",
    "response = requests.post(\"https://ncar.gov.sa/api/index.php/api/documents/all_release_orgnizations/1/\"+str(partial_data['dataLength']), headers=headers, cookies=cookies)\n",
    "all_release_orgnizations = response.json()\n",
    "print('pulled data:', len(all_release_orgnizations['data']))\n",
    "\n",
    "\n",
    "with open('all_release_orgnizations.json', 'w', encoding='utf8') as f:\n",
    "    json.dump(all_release_orgnizations, f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if os.path.exists('data.json'):\n",
    "    print(\"loading existing data.json ...\")\n",
    "    print(\"PLEASE delete it if you want to scrape new fresh data\")\n",
    "    with open('data.json', 'r', encoding='utf8') as f:\n",
    "        pages_data = json.load(f)\n",
    "else:\n",
    "    response = requests.get(\"https://ncar.gov.sa/api/index.php/api/documents/list/1/1/approveDate/DESC\", headers=headers, cookies=cookies)\n",
    "    partial_data = response.json()\n",
    "    print('partial_data', len(partial_data), partial_data)\n",
    "\n",
    "    response = requests.get(\"https://ncar.gov.sa/api/index.php/api/documents/list/1/\"+str(partial_data['dataLength'])+\"/approveDate/DESC\", headers=headers, cookies=cookies)\n",
    "    pages_data = response.json()\n",
    "    print('pulled data:', len(pages_data['data']))\n",
    "\n",
    "\n",
    "    with open('data.json', 'w', encoding='utf8') as f:\n",
    "        json.dump(pages_data, f, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# in reality the \"number\" is the id of the document, and the \"id\" is a randomly generated string for the URLs\n",
    "global_new_id2pagedata = {}\n",
    "global_stupid_id2new_id = {}\n",
    "\n",
    "# i = 0\n",
    "# item = pages_data['data'][i]\n",
    "\n",
    "\n",
    "with open(\"pages_data_populated.json\", \"r\", encoding='utf8') as f:\n",
    "    pages_data = json.load(f)\n",
    "    print(\"LOADED OLD MODEL, PLEASE DELETE IF YOU WANT TO RE-SCRAPE\")\n",
    "\n",
    "failures = []\n",
    "for page_data in pages_data['data']:\n",
    "    new_id = gen_id(page_data)\n",
    "    if new_id in global_new_id2pagedata:\n",
    "        print(f\"DUPLICATE NEW ID: {new_id}\")\n",
    "\n",
    "    page_data['new_id'] = new_id\n",
    "    page_data['pdfPath'] = \"pdfs/\" + (page_data['new_id']) + '.pdf'\n",
    "    global_new_id2pagedata[page_data['new_id']] = page_data\n",
    "    global_stupid_id2new_id[page_data['id']] = new_id\n",
    "\n",
    "    for documentRelation in page_data['documentData']['data']['documentRelation']:\n",
    "        try:\n",
    "            new_id = gen_id(documentRelation)\n",
    "            if new_id in global_new_id2pagedata:\n",
    "                print(f\"DUPLICATE NEW ID: {new_id}\")\n",
    "\n",
    "            documentRelation['new_id'] = new_id\n",
    "            global_new_id2pagedata[documentRelation['new_id']] = documentRelation\n",
    "            global_stupid_id2new_id[documentRelation['id']] = new_id\n",
    "        except Exception as e:\n",
    "            failures.append((page_data, documentRelation, e))\n",
    "            print(f\"Failed to process {documentRelation} due to {e}\")\n",
    "\n",
    "\n",
    "def populate_page_data(page_data):\n",
    "    # populate the page_data with \n",
    "    page_data['pageUrl'] = \"https://ncar.gov.sa/document-details/\" + page_data['id']\n",
    "    page_data['pdfUrl'] = \"https://ncar.gov.sa/api/index.php/resource/\" + page_data['id'] + \"/Documents/OriginalAttachPath\"\n",
    "\n",
    "    if 'documentData' in page_data and page_data['documentData'] is not None:\n",
    "        return\n",
    "    \n",
    "    retries = 5\n",
    "    \n",
    "    for retry in range(retries):\n",
    "        try:\n",
    "            response = requests.get(\"https://ncar.gov.sa/api/index.php/api/documents/document/\" + page_data['id'], headers=headers)\n",
    "            assert response.status_code == 200, response.text\n",
    "            page_data['documentData'] = response.json()\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to get page data for {page_data['id']} on retry {retry} due to {e}\")\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"Failed to get page data for {page_data['id']} after {retries} retries\")\n",
    "        return\n",
    "\n",
    "with ThreadPool(20) as pool:\n",
    "    _ = list(tqdm(pool.imap(populate_page_data, pages_data['data']), 'populating pages data', len(pages_data['data'])))\n",
    "\n",
    "\n",
    "\n",
    "pages_data['meta'] = {\n",
    "    \"scrape_date\": datetime.datetime.now().isoformat(),\n",
    "}\n",
    "with open(\"pages_data_populated.json\", \"w\", encoding='utf8') as f:\n",
    "    json.dump(pages_data, f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from PyPDF2 import PdfReader\n",
    "# from PyPDF2.errors import PdfReadError\n",
    "\n",
    "# # Specify the directory containing the PDF files\n",
    "# directory = \"/home/fhijazi/Projects/LegalGPT/LegalData/scrapers/ncar.gov.sa/pdfs/\"\n",
    "\n",
    "# # Iterate through each file in the directory\n",
    "# for filename in os.listdir(directory):\n",
    "#     if filename.endswith(\".pdf\"):  # Check if the file is a PDF\n",
    "#         file_path = os.path.join(directory, filename)\n",
    "#         # check if size is < 1KB\n",
    "#         if os.path.getsize(file_path) < 1024:\n",
    "#             print(f\"{filename} is too small. Deleting...\")\n",
    "#             os.remove(file_path)\n",
    "#         try:\n",
    "#             # Attempt to open and read the PDF file\n",
    "#             # print('reading', file_path)\n",
    "#             with open(file_path, \"rb\") as file:\n",
    "#                 reader = PdfReader(file)\n",
    "#                 assert (reader.numPages) > 0, 'got no pages'\n",
    "#                 # print(f\"{filename} - Number of pages: {len(reader.pages)}\")\n",
    "#         except Exception as e:\n",
    "#             # If there is an error, delete the PDF file\n",
    "#             print(f\"{filename} is corrupt. Deleting...\")\n",
    "#             os.remove(file_path)\n",
    "#         # except Exception as e:\n",
    "#         #     # Handle other exceptions\n",
    "#         #     print(f\"An error occurred with {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(failures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(global_new_id2pagedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(global_stupid_id2new_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_new_id2stupid_id = {v: k for k, v in global_stupid_id2new_id.items()}\n",
    "len(global_new_id2stupid_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install graph-tool networkx pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_data['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('pages_data_populated.json', 'r', encoding='utf8') as f:\n",
    "    pages_data = json.load(f)\n",
    "\n",
    "id2pdfUrl = {x['new_id']: x['pdfUrl'] for x in pages_data['data']}\n",
    "len(id2pdfUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # jupyter notebook visualize graph.html file\n",
    "# from IPython.display import IFrame, display\n",
    "# # IFrame(src='graph.html', width=900, height=600)\n",
    "\n",
    "# display(HTML('graph.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/visualizing-networks-in-python-d70f4cbeb259"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvis\n",
    "from pyvis.network import Network\n",
    "# create vis network\n",
    "net = Network(notebook=True)\n",
    "# load the networkx graph\n",
    "net.from_nx(G)\n",
    "# show\n",
    "net.show(\"example.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_data['data'][0]['documentData']['data']['documentRelation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# length of related documents for each page_data\n",
    "plt.hist(\n",
    "    [\n",
    "        len(page_data['documentData']['data']['documentRelation'])\n",
    "        for page_data in pages_data['data']\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x['documentData']['data']['id'] for x in pages_data['data']][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open(\"pages_data_populated.json\", \"w\", encoding='utf8') as f:\n",
    "#     json.dump(pages_data, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashmap = {\n",
    "    item['number']: item\n",
    "    for item in pages_data['data']\n",
    "}\n",
    "len(hashmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_data['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashmap = {\n",
    "    hash_json({\n",
    "        \"title_en\": item['title_en'],\n",
    "        \"title_ar\": item['title_ar'],\n",
    "        \"number\": item['number'],\n",
    "        \"approve_date\": item['approve_date'],\n",
    "        # \"summery_ar\": item['documentData']['data']['summery_ar'],\n",
    "        # \"omAlQourah_version\": item['documentData']['data']['omAlQourah_version'],\n",
    "        # \"documentRelation\": json_remove_keys(item['documentData']['data']['documentRelation'], ['id']),\n",
    "    }): item\n",
    "    for item in pages_data['data']\n",
    "}\n",
    "len(hashmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = pages_data['data'][0]\n",
    "json_remove_keys(item, ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    pages_data['data']\n",
    ")\n",
    "# df['hash'] = df.apply(lambda item: hash_json({\n",
    "#     \"title_en\": item['title_en'],\n",
    "#     \"title_ar\": item['title_ar'],\n",
    "#     \"number\": item['number'],\n",
    "#     \"approve_date\": item['approve_date'],\n",
    "#     \"summery_ar\": item['documentData']['data']['summery_ar'],\n",
    "#     \"omAlQourah_version\": item['documentData']['data']['omAlQourah_version'],\n",
    "#     \"documentRelation\": json_remove_keys(item['documentData']['data']['documentRelation'], ['id']),\n",
    "# }), axis=1)\n",
    "\n",
    "\n",
    "df['hash'] = df.apply(lambda item: hash_json(json_remove_keys(item['documentData']['data']['documentRelation'], ['id'])), axis=1)\n",
    "\n",
    "groupby_result = df.groupby(by='hash')\n",
    "# print all ids with group > 1\n",
    "for key, group in groupby_result:\n",
    "    if len(group) > 1:\n",
    "        # print(key, group['id'].values)\n",
    "        print(key)\n",
    "        # print(group[['title_ar', 'title_en']])\n",
    "        print('---')\n",
    "        for i in range(len(group['id'].values)):\n",
    "            print('    ', i, \"https://ncar.gov.sa/document-details/\" + str(group['id'].values[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
