{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "762e5484",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/low_level/evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa530a77-b0a7-46f1-bb17-0cfa31650c60",
   "metadata": {},
   "source": [
    "# Building Evaluation from Scratch\n",
    "\n",
    "We show how you can build evaluation modules from scratch. This includes both evaluation of the final generated response (where the output is plain text), as well as the evaluation of retrievers (where the output is a ranked list of items).\n",
    "\n",
    "We have in-house modules in our [Evaluation](https://gpt-index.readthedocs.io/en/latest/core_modules/supporting_modules/evaluation/root.html) section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69705b2b-38f4-471e-bccf-f3bac26f1582",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We load some data and define a very simple RAG query engine that we'll evaluate (uses top-k retrieval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeaaad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-readers-file pymupdf\n",
    "# %pip install llama-index-llms-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bec520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9883cde-bf3e-40a7-8a2e-59f39daeadc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter, SimpleNodeParser\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402c7ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load yaml file\n",
    "import yaml\n",
    "from easydict import EasyDict\n",
    "config = EasyDict(yaml.safe_load(open(\"defaults.yaml\")))\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6a360d-ea79-444e-9f0f-cfcca9fb9642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "if os.path.exists('sources/ArabicMMLU.csv'):\n",
    "    arabicmmlu_df = pd.read_csv('sources/ArabicMMLU.csv')\n",
    "else:\n",
    "    arabicMMLU = load_dataset('MBZUAI/ArabicMMLU')\n",
    "    arabicmmlu_df = arabicMMLU['test'].to_pandas()\n",
    "    arabicmmlu_df.to_csv('source/ArabicMMLU.csv', index=False)\n",
    "    \n",
    "\n",
    "arabicmmlu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b422d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = SimpleNodeParser.from_defaults(chunk_size=1e6)\n",
    "\n",
    "\n",
    "\n",
    "def convert_qa_to_string(row) -> str:\n",
    "    options = list(map(str.strip, filter(lambda x: x not in [None, np.nan, 'nan'], [\n",
    "       str(row[f\"Option {i}\"]) for i in range(1, 6)\n",
    "    ])))\n",
    "    options_str = \"\\n\".join(\n",
    "        f\"{i+1}. {x}\" for i, x in enumerate(options)\n",
    "    )\n",
    "    answer_key_map = {\n",
    "        \"A\": 0,\n",
    "        \"B\": 1,\n",
    "        \"C\": 2,\n",
    "        \"D\": 3,\n",
    "        \"E\": 4,\n",
    "    }\n",
    "    correct_answer_str = options[answer_key_map[row[\"Answer Key\"]]]\n",
    "\n",
    "    return config.MCQ_PROMPT.format(\n",
    "        question=row['Question'],\n",
    "        options=options_str,\n",
    "        correct_answer=correct_answer_str\n",
    "    )\n",
    "\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        text=convert_qa_to_string(row),\n",
    "        metadata=row.to_dict()\n",
    "    ) for _, row in arabicmmlu_df.query('Subject==\"Law\"').iterrows()\n",
    "]\n",
    "\n",
    "arabicmmlu_nodes = node_parser.get_nodes_from_documents(\n",
    "    documents,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287da53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "moj_df = pd.read_csv('sources/MOJ_Regulations.csv')\n",
    "print(moj_df.shape)\n",
    "moj_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5d757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tameem_df = pd.read_csv('sources/tameem.csv')\n",
    "print(tameem_df.shape)\n",
    "tameem_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478f852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "moj_docs = [ \n",
    "            Document(\n",
    "                text=row['Description'],\n",
    "                metadata=row.drop('Description').to_dict()\n",
    "            ) for _, row in moj_df.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tameem_docs = [\n",
    "    Document(\n",
    "        text=row['نص التعميم'],\n",
    "        metadata=row.drop('نص التعميم').to_dict()\n",
    "    ) for _, row in tameem_df.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaba424",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_docs = moj_docs + tameem_docs\n",
    "len(source_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eada386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama index format\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.llms.anthropic import Anthropic\n",
    "AZURE_OPENAI_KEY = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "CLAUDE_API_KEY = os.getenv(\"CLAUDE_API_KEY\")\n",
    "api_version = \"2024-02-15-preview\"\n",
    "\n",
    "llm_gpt4 = AzureOpenAI(\n",
    "    engine=\"gpt-4\",\n",
    "    api_key=AZURE_OPENAI_KEY,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_version=api_version,\n",
    ")\n",
    "\n",
    "llm_claude3 = Anthropic(\n",
    "    'claude-3-opus-20240229',\n",
    "    api_key=CLAUDE_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de6d37e-dc23-43a2-9d47-605d5978c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = SentenceSplitter(chunk_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4868a75-0a1c-486c-81b1-bea989e62591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "OPENAI_API_KEY= os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from llama_index.core import Settings\n",
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import PromptTemplate, ServiceContext, StorageContext, VectorStoreIndex, load_index_from_storage\n",
    "\n",
    "embed_model = OpenAIEmbedding(model='text-embedding-3-large', api_key=OPENAI_API_KEY)\n",
    "\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138aae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm_claude3\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path='./chroma_db')\n",
    "# Traditional VDB\n",
    "try:\n",
    "    chroma_collection = chroma_client.get_collection(f'ArabicMMLU_legal')\n",
    "except Exception as e:\n",
    "    print(\"Creating new collection\")\n",
    "    chroma_collection = chroma_client.create_collection('ArabicMMLU_legal')\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "arabicmmlu_index = VectorStoreIndex(\n",
    "    arabicmmlu_nodes,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=embed_model,\n",
    "    use_async=False,\n",
    "    show_progress=True,\n",
    ")\n",
    "# arabicmmlu_retriever = arabicmmlu_index.as_retriever(\n",
    "#     similarity_top_k=3,\n",
    "#     embed_model=embed_model,\n",
    "# )\n",
    "# # Sentence window retrieval\n",
    "# query_engine_sentence_window = index_sentence_window.as_query_engine(\n",
    "#     text_qa_template=text_qa_template, similarity_top_k=3, embed_model=embed_model, llm=llm\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ea82ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = SentenceSplitter(chunk_size=1e6, chunk_overlap=0)\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(\n",
    "    source_docs,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b60e745-aae4-4ab2-be7d-563a883d27a3",
   "metadata": {},
   "source": [
    "## Dataset Generation\n",
    "\n",
    "We first go through an exercise of generating a synthetic evaluation dataset. We do this by synthetically generating a set of questions from existing context. We then run each question with existing context through a powerful LLM (e.g. GPT-4) to generate a \"ground-truth\" response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b4f0bf-8f7d-4693-b055-d050020b1291",
   "metadata": {},
   "source": [
    "### Define Functions\n",
    "\n",
    "We define the functions that we will use for dataset generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a305907-14d1-4f5c-a15c-145ae7dcb3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import BaseNode\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core import ChatPromptTemplate, PromptTemplate\n",
    "from typing import Tuple, List\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "from multiprocessing.pool import ThreadPool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70019ed9-bed5-434e-b6d3-545726ac7397",
   "metadata": {},
   "source": [
    "We define `generate_answers_for_questions` to generate answers from questions given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca84683-5d4c-4b1f-ae4d-cd6aea51dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_template = ChatPromptTemplate(\n",
    "    message_templates=[\n",
    "        ChatMessage(role=MessageRole.USER, content=config.QA_PROMPT),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def generate_answers_for_questions(\n",
    "    questions: List[str], context: str, llm: OpenAI\n",
    ") -> str:\n",
    "    \"\"\"Generate answers for questions given context.\"\"\"\n",
    "    \n",
    "    def generate_answer(idx, question):\n",
    "        fmt_qa_prompt = question_answer_template.format_messages(\n",
    "            context_str=context,\n",
    "            query_str=question,\n",
    "        )\n",
    "        response_obj = llm.chat(fmt_qa_prompt)\n",
    "        return response_obj.message.content\n",
    "\n",
    "    # for idx, node in enumerate(nodes):\n",
    "    answers = list(\n",
    "        tqdm(\n",
    "            ThreadPool().imap(\n",
    "                lambda x: generate_answer(*x),\n",
    "                enumerate(questions),\n",
    "            ),\n",
    "        \"generate_answers_for_questions()\",\n",
    "        total=len(questions),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c0e168-e2d4-4e63-ae77-5413499b0c7d",
   "metadata": {},
   "source": [
    "We define `generate_qa_pairs` to generate qa pairs over an entire list of Nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b4e96d-d08d-4c69-9b35-0dbb1f0a61b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_gen_template = ChatPromptTemplate(\n",
    "    message_templates=[\n",
    "        ChatMessage(role=MessageRole.SYSTEM, content=config.QUESTION_GEN_SYS_TMPL),\n",
    "        ChatMessage(role=MessageRole.USER, content=config.QUESTION_GEN_USER_TMPL),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def generate_qa_pairs(\n",
    "    nodes: List[BaseNode], llm: OpenAI, num_questions_per_chunk: int = 2,\n",
    "    delimiter: str = \"\\n\",\n",
    "    question_gen_template=question_gen_template,\n",
    ") -> List[Tuple[str, str]]:\n",
    "    \"\"\"Generate questions.\"\"\"\n",
    "    def process_node(idx, node):\n",
    "        context_str = node.get_content(metadata_mode=\"all\")\n",
    "        fmt_messages = question_gen_template.format_messages(\n",
    "            num_questions_per_chunk=num_questions_per_chunk,\n",
    "            context_str=context_str,\n",
    "        )\n",
    "        chat_response = llm.chat(fmt_messages)\n",
    "        raw_output = chat_response.message.content\n",
    "\n",
    "        result_list = str(raw_output).strip().split(delimiter)\n",
    "        cleaned_questions = [\n",
    "            re.sub(r\"^\\d+[\\).\\s]\", \"\", question).strip()\n",
    "            for question in result_list\n",
    "        ]\n",
    "        answers = generate_answers_for_questions(\n",
    "            cleaned_questions, context_str, llm\n",
    "        )\n",
    "        cur_qa_pairs = list(zip(cleaned_questions, answers))\n",
    "        return cur_qa_pairs\n",
    "    \n",
    "    qa_pairs = list(\n",
    "        tqdm(\n",
    "            ThreadPool().imap(\n",
    "                lambda x: process_node(*x),\n",
    "                enumerate(nodes),\n",
    "            ),\n",
    "        \"Generating QA pairs\",\n",
    "        total=len(nodes),\n",
    "        )\n",
    "    )\n",
    "    # flatten\n",
    "    qa_pairs = [item for sublist in qa_pairs for item in sublist]\n",
    "        \n",
    "    return qa_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30455ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = generate_qa_pairs(\n",
    "    nodes,\n",
    "    llm=llm_gpt4,\n",
    "    num_questions_per_chunk=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d98305",
   "metadata": {},
   "source": [
    "Converting question answer paris int MSQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_to_mcq_template = ChatPromptTemplate(\n",
    "    message_templates=[\n",
    "        ChatMessage(role=MessageRole.USER, content=config.QA_TO_MCQ_PROMPT),\n",
    "    ]\n",
    ")\n",
    "qa_to_mcq_cot_template = ChatPromptTemplate(\n",
    "    message_templates=[\n",
    "        ChatMessage(role=MessageRole.USER, content=config.QA_TO_MCQ_COT_PROMPT),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb47b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_quesitons_to_mcqs(\n",
    "    qa_pairs: List[tuple], mcq_prompt_template: str, llm: OpenAI\n",
    ") -> str:\n",
    "    \"\"\"Converting question-answer paris into MCQs.\"\"\"\n",
    "    \n",
    "    def question_to_mcq(idx, qa_pair):\n",
    "        question, answer = qa_pair\n",
    "        prompt_template = mcq_prompt_template.format_messages(\n",
    "            question=question,\n",
    "            answer=answer,\n",
    "        )\n",
    "        response_obj = llm.chat(prompt_template)\n",
    "        return response_obj.message.content\n",
    "\n",
    "    mcqs = list(\n",
    "        tqdm(\n",
    "            ThreadPool().imap(\n",
    "                lambda x: question_to_mcq(*x),\n",
    "                enumerate(qa_pairs),\n",
    "            ),\n",
    "        \"convert_quesitons_to_mcqs()\",\n",
    "        total=len(qa_pairs),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return mcqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349f1324",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcqs = convert_quesitons_to_mcqs(\n",
    "    qa_pairs,\n",
    "    qa_to_mcq_template,\n",
    "    llm_gpt4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b541f261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_mcqs(mcqs):\n",
    "    formatted_mcqs = []\n",
    "    for example in mcqs:\n",
    "        question = example.split('\\n')[0]\n",
    "        options = example.split('\\n')[2:]\n",
    "        formatted_mcqs.append([question, options])\n",
    "    return formatted_mcqs\n",
    "\n",
    "formated_mcqs = format_mcqs(mcqs)\n",
    "formated_mcqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ecd9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_to_mcqs_dict = {}\n",
    "for i in range(len(formated_mcqs)):\n",
    "    qa_to_mcqs_dict[i] = {\n",
    "        'question': formated_mcqs[i][0],\n",
    "        'options': formated_mcqs[i][1],\n",
    "        'answer': formated_mcqs[i][1][0],\n",
    "        'context': nodes[i].text\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9c11e0",
   "metadata": {},
   "source": [
    "With Chain of Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4664ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcqs_cot = convert_quesitons_to_mcqs(\n",
    "    qa_pairs,\n",
    "    qa_to_mcq_cot_template,\n",
    "    llm_gpt4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a40849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_mcqs_cot(mcqs_cot):\n",
    "    formated_mcqs_cot = []\n",
    "    for example in mcqs_cot:\n",
    "        example = example.strip('<start_thought>').split('<end_thought>')\n",
    "\n",
    "        reasoning = example[0]\n",
    "        question = example[1].strip().split('\\n')[0]\n",
    "        options = example[1].strip().split('\\n')[2:]    \n",
    "        \n",
    "        answer = options[0]\n",
    "        formated_mcqs_cot.append([question, reasoning, options, answer])\n",
    "        \n",
    "    return formated_mcqs_cot\n",
    "\n",
    "formated_mcqs_cot = format_mcqs_cot(mcqs_cot)\n",
    "formated_mcqs_cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7abd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_to_mcqs_cot_dict = {}\n",
    "for i in range(len(formated_mcqs_cot)):\n",
    "    qa_to_mcqs_cot_dict[i] = {\n",
    "        'question': formated_mcqs_cot[i][0],\n",
    "        'reasoning': formated_mcqs_cot[i][1],\n",
    "        'options': formated_mcqs_cot[i][2],\n",
    "        'answer': formated_mcqs_cot[i][2][0],\n",
    "        'context': nodes[i].text\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c3abef",
   "metadata": {},
   "source": [
    "#### For MCQ Generation Using In-context Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37459d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "mcq_question_gen_template = ChatPromptTemplate(\n",
    "    message_templates=[\n",
    "        ChatMessage(role=MessageRole.SYSTEM, content=config.MCQ_QUESTION_GEN_SYS_TMPL),\n",
    "        ChatMessage(role=MessageRole.USER, content=config.MCQ_QUESTION_GEN_USER_TMPL),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def generate_mcq_pairs(\n",
    "    nodes_dict: dict,\n",
    "    num_questions_per_chunk: int = 2,\n",
    "    top_k: int = 3,\n",
    "    delimiter: str = \"####\",\n",
    "    mcq_question_gen_template=mcq_question_gen_template,\n",
    "    \n",
    ") -> List[Tuple[str, str]]:\n",
    "    \"\"\"Generate questions.\"\"\"\n",
    "    engine, nodes = list(nodes_dict.items())[0]\n",
    "    if engine == 'gpt-4':\n",
    "        llm = llm_gpt4\n",
    "    else:\n",
    "        llm = llm_claude3\n",
    "    def process_node(idx, node):\n",
    "        cur_context_str = node.get_content(metadata_mode=\"none\")\n",
    "        if \"{few_shot_examples}\" in '\\n'.join([x.content for x in mcq_question_gen_template.message_templates]) and top_k > 0:\n",
    "            arabicmmlu_retriever = arabicmmlu_index.as_retriever(\n",
    "                similarity_top_k=top_k,\n",
    "                embed_model=embed_model,\n",
    "            )\n",
    "            few_shot_examples_str = \"\\n\\n\".join([\n",
    "                x.text for x in arabicmmlu_retriever.retrieve(node.text)\n",
    "            ])\n",
    "        else:\n",
    "            few_shot_examples_str = \"\"\n",
    "\n",
    "        fmt_messages = mcq_question_gen_template.format_messages(\n",
    "            num_questions_per_chunk=num_questions_per_chunk,\n",
    "            context_str=cur_context_str,\n",
    "            few_shot_examples=few_shot_examples_str,\n",
    "        )\n",
    "        try:\n",
    "            chat_response = llm.chat(fmt_messages)\n",
    "            raw_output = chat_response.message.content\n",
    "        except Exception as e:\n",
    "            # add a 2 second sleep and retry\n",
    "            time.sleep(2)\n",
    "            try:\n",
    "                chat_response = llm.chat(fmt_messages)\n",
    "                raw_output = chat_response.message.content\n",
    "            except Exception as e:\n",
    "                raw_output = ''\n",
    "        result_list = str(raw_output).strip().split(delimiter)\n",
    "        # cur_mcq_pairs = [\n",
    "        #     #TODO: make this from the config\n",
    "        #     question.strip().strip('السؤال: ').split(\"الجواب الصحيح: \")\n",
    "        #     for question in result_list if question.strip()\n",
    "        # ]\n",
    "        cur_mcq_pairs = [f'Engine: {engine}' + '\\n\\n' + f'Context: {node.text}\\n\\n' + x.strip() for x in result_list if x.strip()]\n",
    "        \n",
    "        return cur_mcq_pairs\n",
    "    \n",
    "    mcq_pairs = []\n",
    "    batch_size = 10\n",
    "    for i in range(0, len(nodes), batch_size):\n",
    "        mcq_pairs += list(\n",
    "        tqdm(\n",
    "            ThreadPool().imap(\n",
    "                lambda x: process_node(*x),\n",
    "                enumerate(nodes[i:i+batch_size]),\n",
    "            ),\n",
    "        f\"Generating {engine} QA pairs\",\n",
    "        total=batch_size,\n",
    "            )\n",
    "        )\n",
    "        with open(f\"{engine}_MCQs.json\", \"w\") as f:\n",
    "            json.dump(mcq_pairs, f, indent=4, ensure_ascii=False)\n",
    "    # flatten\n",
    "    mcq_pairs = [item for sublist in mcq_pairs for item in sublist]\n",
    "    return mcq_pairs\n",
    "\n",
    "# mcq_pairs = generate_mcq_pairs(\n",
    "#     random_nodes,\n",
    "#     # nodes,\n",
    "#     llm,\n",
    "#     mcq_question_gen_template=mcq_question_gen_template,\n",
    "#     num_questions_per_chunk=4,\n",
    "#     delimiter=\"####\"\n",
    "# )\n",
    "# for q, a in mcq_pairs:\n",
    "#     print(f\"Q: {q}\\nA: {a}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b26051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling the nodes with a fixed seed\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7745e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = int(len(nodes)*0.5)\n",
    "gpt4_nodes = nodes[:i]\n",
    "claude3_nodes = nodes[i:]\n",
    "all_nodes = [{'gpt-4': gpt4_nodes}, {'claude-3-opus': claude3_nodes}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f160ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gpt4_nodes), len(claude3_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8333691",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_shot_mcq = generate_mcq_pairs(\n",
    "                                nodes, \n",
    "                                mcq_question_gen_template=mcq_question_gen_template,\n",
    "                                num_questions_per_chunk=2,\n",
    "                                top_k=3,\n",
    "                                delimiter=\"####\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thiqatiBot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
